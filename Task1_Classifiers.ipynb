{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R5PwTF9bTmgX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = os.path.join(root_path, \"LFW_Dataset/\")\n",
    "\n",
    "## Results from CycleGAN\n",
    "cycle_gan_folder = \"pytorch-CycleGAN-and-pix2pix\"\n",
    "dataset = \"LFW Dataset\"\n",
    "model_trained = \"mask2unmask_final2\"\n",
    "\n",
    "trainA_path = os.path.join(root_path, cycle_gan_folder, \"datasets\", \"LFW_dataset\", \"trainA\")\n",
    "trainB_path = os.path.join(root_path, cycle_gan_folder, \"datasets\", \"LFW_dataset\", \"trainB\")\n",
    "\n",
    "testA_path = os.path.join(root_path, cycle_gan_folder, \"datasets\", \"LFW_dataset\", \"testA\")\n",
    "testB_path = os.path.join(root_path, cycle_gan_folder, \"datasets\", \"LFW_dataset\", \"testB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(folder_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "        for name in files:\n",
    "            paths.append((root, name))\n",
    "    return paths\n",
    "    \n",
    "def create_df_from_real(paths):\n",
    "    df = pd.DataFrame(paths, columns=['root', 'filename'])\n",
    "    \n",
    "    df['fullpath'] = df['root'] + '/' + df['filename']\n",
    "    df = df.drop(columns=\"root\")\n",
    "    df['person'] = df['filename'].apply(lambda x: \" \".join(x.split(\"_\")[:-1]))\n",
    "    \n",
    "    df[\"name\"] = df[\"filename\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting results from CycleGAN\n",
    "\n",
    "trainA_urls = extract_files(trainA_path)\n",
    "trainB_urls = extract_files(trainA_path)\n",
    "\n",
    "testA_urls = extract_files(testA_path)\n",
    "testB_urls = extract_files(testB_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainA  = create_df_from_real(trainA_urls)\n",
    "trainB  = create_df_from_real(trainB_urls)\n",
    "\n",
    "testA  = create_df_from_real(testA_urls)\n",
    "testB  = create_df_from_real(testB_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Scott McClellan\n",
       "1             Alvaro Uribe\n",
       "2            Roger Federer\n",
       "3             Bertie Ahern\n",
       "4            Joe Lieberman\n",
       "               ...        \n",
       "1264            JK Rowling\n",
       "1265          Roh Moo-hyun\n",
       "1266       Renee Zellweger\n",
       "1267    Rubens Barrichello\n",
       "1268         Tung Chee-hwa\n",
       "Name: person, Length: 1269, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainB[\"person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "PZ_XSD5qTnbV",
    "outputId": "466d2f66-1245-41a7-a792-a8d3e4c68193"
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Ouf1oBXMT5M1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(.5, 1.)),\n",
    "        transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, target_transform=None,\n",
    "                 path_col = 'fullpath', label_col = 'person'):\n",
    "        self.df = df.copy()\n",
    "        self.img_paths = df[path_col]\n",
    "        self.labels = df[label_col]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_paths.iloc[idx]\n",
    "        image = Image.open(path)\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def tokenize_names(name: str):\n",
    "    return name_to_idx[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "current_path = os.getcwd()\n",
    "tokenz_path = os.path.join(current_path, \"tokens.json\")\n",
    "f = open(tokenz_path)\n",
    "name_to_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abdullah Gul': 0,\n",
       " 'Adrien Brody': 1,\n",
       " 'Ahmed Chalabi': 2,\n",
       " 'Ai Sugiyama': 3,\n",
       " 'Al Gore': 4,\n",
       " 'Al Sharpton': 5,\n",
       " 'Alan Greenspan': 6,\n",
       " 'Alastair Campbell': 7,\n",
       " 'Albert Costa': 8,\n",
       " 'Alejandro Toledo': 9,\n",
       " 'Ali Naimi': 10,\n",
       " 'Allyson Felix': 11,\n",
       " 'Alvaro Uribe': 12,\n",
       " 'Amelia Vega': 13,\n",
       " 'Amelie Mauresmo': 14,\n",
       " 'Ana Guevara': 15,\n",
       " 'Ana Palacio': 16,\n",
       " 'Andre Agassi': 17,\n",
       " 'Andy Roddick': 18,\n",
       " 'Angela Bassett': 19,\n",
       " 'Angela Merkel': 20,\n",
       " 'Angelina Jolie': 21,\n",
       " 'Ann Veneman': 22,\n",
       " 'Anna Kournikova': 23,\n",
       " 'Antonio Banderas': 24,\n",
       " 'Antonio Palocci': 25,\n",
       " 'Ari Fleischer': 26,\n",
       " 'Ariel Sharon': 27,\n",
       " 'Arminio Fraga': 28,\n",
       " 'Arnold Schwarzenegger': 29,\n",
       " 'Arnoldo Aleman': 30,\n",
       " 'Ashanti': 31,\n",
       " 'Atal Bihari Vajpayee': 32,\n",
       " 'Ben Affleck': 33,\n",
       " 'Benazir Bhutto': 34,\n",
       " 'Benjamin Netanyahu': 35,\n",
       " 'Bernard Law': 36,\n",
       " 'Bertie Ahern': 37,\n",
       " 'Bill Clinton': 38,\n",
       " 'Bill Frist': 39,\n",
       " 'Bill Gates': 40,\n",
       " 'Bill Graham': 41,\n",
       " 'Bill McBride': 42,\n",
       " 'Bill Simon': 43,\n",
       " 'Billy Crystal': 44,\n",
       " 'Binyamin Ben-Eliezer': 45,\n",
       " 'Bob Graham': 46,\n",
       " 'Bob Hope': 47,\n",
       " 'Bob Stoops': 48,\n",
       " 'Boris Becker': 49,\n",
       " 'Brad Johnson': 50,\n",
       " 'Britney Spears': 51,\n",
       " 'Bulent Ecevit': 52,\n",
       " 'Calista Flockhart': 53,\n",
       " 'Cameron Diaz': 54,\n",
       " 'Carla Del Ponte': 55,\n",
       " 'Carlos Menem': 56,\n",
       " 'Carlos Moya': 57,\n",
       " 'Carmen Electra': 58,\n",
       " 'Carrie-Anne Moss': 59,\n",
       " 'Catherine Deneuve': 60,\n",
       " 'Catherine Zeta-Jones': 61,\n",
       " 'Celine Dion': 62,\n",
       " 'Cesar Gaviria': 63,\n",
       " 'Chanda Rubin': 64,\n",
       " 'Charles Moose': 65,\n",
       " 'Charles Taylor': 66,\n",
       " 'Charlton Heston': 67,\n",
       " 'Chen Shui-bian': 68,\n",
       " 'Choi Sung-hong': 69,\n",
       " 'Christine Baumgartner': 70,\n",
       " 'Christine Todd Whitman': 71,\n",
       " 'Ciro Gomes': 72,\n",
       " 'Clara Harris': 73,\n",
       " 'Claudia Pechstein': 74,\n",
       " 'Clay Aiken': 75,\n",
       " 'Clint Eastwood': 76,\n",
       " 'Colin Farrell': 77,\n",
       " 'Colin Montgomerie': 78,\n",
       " 'Colin Powell': 79,\n",
       " 'Condoleezza Rice': 80,\n",
       " 'Costas Simitis': 81,\n",
       " 'Cruz Bustamante': 82,\n",
       " 'David Anderson': 83,\n",
       " 'David Beckham': 84,\n",
       " 'David Heymann': 85,\n",
       " 'David Nalbandian': 86,\n",
       " 'David Trimble': 87,\n",
       " 'David Wells': 88,\n",
       " 'Dennis Hastert': 89,\n",
       " 'Dennis Kucinich': 90,\n",
       " 'Denzel Washington': 91,\n",
       " 'Diana Krall': 92,\n",
       " 'Dick Cheney': 93,\n",
       " 'Dominique de Villepin': 94,\n",
       " 'Donald Rumsfeld': 95,\n",
       " 'Edmund Stoiber': 96,\n",
       " 'Eduard Shevardnadze': 97,\n",
       " 'Eduardo Duhalde': 98,\n",
       " 'Edward Lu': 99,\n",
       " 'Elizabeth Hurley': 100,\n",
       " 'Elizabeth Smart': 101,\n",
       " 'Elsa Zylberstein': 102,\n",
       " 'Elton John': 103,\n",
       " 'Emanuel Ginobili': 104,\n",
       " 'Emma Watson': 105,\n",
       " 'Enrique Bolanos': 106,\n",
       " 'Erika Harold': 107,\n",
       " 'Fernando Gonzalez': 108,\n",
       " 'Fernando Henrique Cardoso': 109,\n",
       " 'Fidel Castro': 110,\n",
       " 'Frank Solich': 111,\n",
       " 'Fujio Cho': 112,\n",
       " 'Gene Robinson': 113,\n",
       " 'Geoff Hoon': 114,\n",
       " 'George Clooney': 115,\n",
       " 'George HW Bush': 116,\n",
       " 'George Lopez': 117,\n",
       " 'George Pataki': 118,\n",
       " 'George Robertson': 119,\n",
       " 'George W Bush': 120,\n",
       " 'Gerhard Schroeder': 121,\n",
       " 'Gerry Adams': 122,\n",
       " 'Gil de Ferran': 123,\n",
       " 'Gloria Macapagal Arroyo': 124,\n",
       " 'Goldie Hawn': 125,\n",
       " 'Gonzalo Sanchez de Lozada': 126,\n",
       " 'Gordon Brown': 127,\n",
       " 'Grant Hackett': 128,\n",
       " 'Gray Davis': 129,\n",
       " 'Gregg Popovich': 130,\n",
       " 'Guillermo Coria': 131,\n",
       " 'Gunter Pleuger': 132,\n",
       " 'Gwyneth Paltrow': 133,\n",
       " 'Habib Rizieq': 134,\n",
       " 'Hal Gehman': 135,\n",
       " 'Halle Berry': 136,\n",
       " 'Hamid Karzai': 137,\n",
       " 'Hans Blix': 138,\n",
       " 'Harrison Ford': 139,\n",
       " 'Heidi Klum': 140,\n",
       " 'Heizo Takenaka': 141,\n",
       " 'Hillary Clinton': 142,\n",
       " 'Hitomi Soga': 143,\n",
       " 'Holly Hunter': 144,\n",
       " 'Hosni Mubarak': 145,\n",
       " 'Howard Dean': 146,\n",
       " 'Hu Jintao': 147,\n",
       " 'Hugh Grant': 148,\n",
       " 'Hugo Chavez': 149,\n",
       " 'Ian Thorpe': 150,\n",
       " 'Igor Ivanov': 151,\n",
       " 'JK Rowling': 152,\n",
       " 'Jack Straw': 153,\n",
       " 'Jackie Chan': 154,\n",
       " 'Jacques Chirac': 155,\n",
       " 'Jacques Rogge': 156,\n",
       " 'Jake Gyllenhaal': 157,\n",
       " 'James Blake': 158,\n",
       " 'James Kelly': 159,\n",
       " 'James Wolfensohn': 160,\n",
       " 'Jan Ullrich': 161,\n",
       " 'Jason Kidd': 162,\n",
       " 'Javier Solana': 163,\n",
       " 'Jay Garner': 164,\n",
       " 'Jean Charest': 165,\n",
       " 'Jean Chretien': 166,\n",
       " 'Jean-David Levitte': 167,\n",
       " 'Jean-Pierre Raffarin': 168,\n",
       " 'Jeb Bush': 169,\n",
       " 'Jelena Dokic': 170,\n",
       " 'Jennifer Aniston': 171,\n",
       " 'Jennifer Capriati': 172,\n",
       " 'Jennifer Garner': 173,\n",
       " 'Jennifer Lopez': 174,\n",
       " 'Jeong Se-hyun': 175,\n",
       " 'Jeremy Greenstock': 176,\n",
       " 'Jesse Jackson': 177,\n",
       " 'Jiang Zemin': 178,\n",
       " 'Jim Furyk': 179,\n",
       " 'Jimmy Carter': 180,\n",
       " 'Jiri Novak': 181,\n",
       " 'Joan Laporta': 182,\n",
       " 'Joe Lieberman': 183,\n",
       " 'John Abizaid': 184,\n",
       " 'John Allen Muhammad': 185,\n",
       " 'John Ashcroft': 186,\n",
       " 'John Bolton': 187,\n",
       " 'John Edwards': 188,\n",
       " 'John Howard': 189,\n",
       " 'John Kerry': 190,\n",
       " 'John Manley': 191,\n",
       " 'John McCain': 192,\n",
       " 'John Negroponte': 193,\n",
       " 'John Paul II': 194,\n",
       " 'John Snow': 195,\n",
       " 'John Stockton': 196,\n",
       " 'John Travolta': 197,\n",
       " 'Jon Gruden': 198,\n",
       " 'Jonathan Edwards': 199,\n",
       " 'Joschka Fischer': 200,\n",
       " 'Jose Manuel Durao Barroso': 201,\n",
       " 'Jose Maria Aznar': 202,\n",
       " 'Jose Serra': 203,\n",
       " 'Joseph Biden': 204,\n",
       " 'Juan Carlos Ferrero': 205,\n",
       " 'Juan Pablo Montoya': 206,\n",
       " 'Julianne Moore': 207,\n",
       " 'Julie Gerberding': 208,\n",
       " 'Junichiro Koizumi': 209,\n",
       " 'Justin Timberlake': 210,\n",
       " 'Justine Pasek': 211,\n",
       " 'Kalpana Chawla': 212,\n",
       " 'Kamal Kharrazi': 213,\n",
       " 'Kate Hudson': 214,\n",
       " 'Keanu Reeves': 215,\n",
       " 'Kevin Costner': 216,\n",
       " 'Kevin Spacey': 217,\n",
       " 'Kim Clijsters': 218,\n",
       " 'Kim Dae-jung': 219,\n",
       " 'Kim Ryong-sung': 220,\n",
       " 'King Abdullah II': 221,\n",
       " 'Kofi Annan': 222,\n",
       " 'Kristanna Loken': 223,\n",
       " 'Kurt Warner': 224,\n",
       " 'Lance Armstrong': 225,\n",
       " 'Lance Bass': 226,\n",
       " 'Larry Brown': 227,\n",
       " 'Laura Bush': 228,\n",
       " 'LeBron James': 229,\n",
       " 'Leonardo DiCaprio': 230,\n",
       " 'Leonid Kuchma': 231,\n",
       " 'Li Peng': 232,\n",
       " 'Li Zhaoxing': 233,\n",
       " 'Lindsay Davenport': 234,\n",
       " 'Liza Minnelli': 235,\n",
       " 'Lleyton Hewitt': 236,\n",
       " 'Lucio Gutierrez': 237,\n",
       " 'Lucy Liu': 238,\n",
       " 'Ludivine Sagnier': 239,\n",
       " 'Luis Ernesto Derbez Bautista': 240,\n",
       " 'Luis Gonzalez Macchi': 241,\n",
       " 'Luis Horna': 242,\n",
       " 'Luiz Inacio Lula da Silva': 243,\n",
       " 'Madonna': 244,\n",
       " 'Mahathir Mohamad': 245,\n",
       " 'Mahmoud Abbas': 246,\n",
       " 'Marcelo Rios': 247,\n",
       " 'Marco Antonio Barrera': 248,\n",
       " 'Maria Shriver': 249,\n",
       " 'Maria Soledad Alvear Valenzuela': 250,\n",
       " 'Mariah Carey': 251,\n",
       " 'Mark Hurlbert': 252,\n",
       " 'Mark Philippoussis': 253,\n",
       " 'Martha Stewart': 254,\n",
       " 'Martin McGuinness': 255,\n",
       " 'Martin Scorsese': 256,\n",
       " 'Martina McBride': 257,\n",
       " 'Matthew Perry': 258,\n",
       " 'Megawati Sukarnoputri': 259,\n",
       " 'Meryl Streep': 260,\n",
       " 'Michael Bloomberg': 261,\n",
       " 'Michael Chang': 262,\n",
       " 'Michael Chiklis': 263,\n",
       " 'Michael Douglas': 264,\n",
       " 'Michael Jackson': 265,\n",
       " 'Michael Phelps': 266,\n",
       " 'Michael Powell': 267,\n",
       " 'Michael Schumacher': 268,\n",
       " 'Michelle Kwan': 269,\n",
       " 'Michelle Yeoh': 270,\n",
       " 'Mick Jagger': 271,\n",
       " 'Mike Krzyzewski': 272,\n",
       " 'Mike Martz': 273,\n",
       " 'Mike Myers': 274,\n",
       " 'Mike Weir': 275,\n",
       " 'Mireya Moscoso': 276,\n",
       " 'Mohamed ElBaradei': 277,\n",
       " 'Mohammad Khatami': 278,\n",
       " 'Mohammed Al-Douri': 279,\n",
       " 'Monica Seles': 280,\n",
       " 'Muhammad Ali': 281,\n",
       " 'Muhammad Saeed al-Sahhaf': 282,\n",
       " 'Nadia Petrova': 283,\n",
       " 'Naji Sabri': 284,\n",
       " 'Nancy Pelosi': 285,\n",
       " 'Naomi Watts': 286,\n",
       " 'Natalie Coughlin': 287,\n",
       " 'Natalie Maines': 288,\n",
       " 'Nestor Kirchner': 289,\n",
       " 'Nia Vardalos': 290,\n",
       " 'Nicanor Duarte Frutos': 291,\n",
       " 'Nick Nolte': 292,\n",
       " 'Nicole Kidman': 293,\n",
       " 'Norah Jones': 294,\n",
       " 'Norm Coleman': 295,\n",
       " 'Oscar De La Hoya': 296,\n",
       " 'Oswaldo Paya': 297,\n",
       " 'Pamela Anderson': 298,\n",
       " 'Paradorn Srichaphan': 299,\n",
       " 'Paul Bremer': 300,\n",
       " 'Paul Burrell': 301,\n",
       " 'Paul Martin': 302,\n",
       " 'Paul McCartney': 303,\n",
       " 'Paul ONeill': 304,\n",
       " 'Paul Wolfowitz': 305,\n",
       " 'Paula Radcliffe': 306,\n",
       " 'Pedro Almodovar': 307,\n",
       " 'Pedro Malan': 308,\n",
       " 'Pervez Musharraf': 309,\n",
       " 'Pete Sampras': 310,\n",
       " 'Peter Struck': 311,\n",
       " 'Pierce Brosnan': 312,\n",
       " 'Prince Charles': 313,\n",
       " 'Princess Caroline': 314,\n",
       " 'Queen Elizabeth II': 315,\n",
       " 'Queen Rania': 316,\n",
       " 'Rainer Schuettler': 317,\n",
       " 'Ralf Schumacher': 318,\n",
       " 'Ray Romano': 319,\n",
       " 'Recep Tayyip Erdogan': 320,\n",
       " 'Renee Zellweger': 321,\n",
       " 'Ricardo Lagos': 322,\n",
       " 'Ricardo Sanchez': 323,\n",
       " 'Richard Armitage': 324,\n",
       " 'Richard Gephardt': 325,\n",
       " 'Richard Gere': 326,\n",
       " 'Richard Myers': 327,\n",
       " 'Richard Virenque': 328,\n",
       " 'Rick Perry': 329,\n",
       " 'Rob Marshall': 330,\n",
       " 'Robert Blake': 331,\n",
       " 'Robert De Niro': 332,\n",
       " 'Robert Duvall': 333,\n",
       " 'Robert Kocharian': 334,\n",
       " 'Robert Mueller': 335,\n",
       " 'Robert Redford': 336,\n",
       " 'Robert Zoellick': 337,\n",
       " 'Roger Federer': 338,\n",
       " 'Roger Moore': 339,\n",
       " 'Roh Moo-hyun': 340,\n",
       " 'Roman Polanski': 341,\n",
       " 'Romano Prodi': 342,\n",
       " 'Ron Dittemore': 343,\n",
       " 'Roy Moore': 344,\n",
       " 'Rubens Barrichello': 345,\n",
       " 'Rudolph Giuliani': 346,\n",
       " 'Russell Simmons': 347,\n",
       " 'Saddam Hussein': 348,\n",
       " 'Salma Hayek': 349,\n",
       " 'Sarah Hughes': 350,\n",
       " 'Sarah Jessica Parker': 351,\n",
       " 'Scott McClellan': 352,\n",
       " 'Scott Peterson': 353,\n",
       " 'Sean OKeefe': 354,\n",
       " 'Serena Williams': 355,\n",
       " 'Sergei Ivanov': 356,\n",
       " 'Sergey Lavrov': 357,\n",
       " 'Sergio Vieira De Mello': 358,\n",
       " 'Sharon Stone': 359,\n",
       " 'Sheryl Crow': 360,\n",
       " 'Shimon Peres': 361,\n",
       " 'Silvan Shalom': 362,\n",
       " 'Silvio Berlusconi': 363,\n",
       " 'Sophia Loren': 364,\n",
       " 'Sourav Ganguly': 365,\n",
       " 'Spencer Abraham': 366,\n",
       " 'Steffi Graf': 367,\n",
       " 'Steve Lavin': 368,\n",
       " 'Steve Nash': 369,\n",
       " 'Steven Spielberg': 370,\n",
       " 'Susan Sarandon': 371,\n",
       " 'Sylvester Stallone': 372,\n",
       " 'Taha Yassin Ramadan': 373,\n",
       " 'Tang Jiaxuan': 374,\n",
       " 'Tariq Aziz': 375,\n",
       " 'Thabo Mbeki': 376,\n",
       " 'Thaksin Shinawatra': 377,\n",
       " 'Thomas OBrien': 378,\n",
       " 'Tiger Woods': 379,\n",
       " 'Tim Henman': 380,\n",
       " 'Tim Robbins': 381,\n",
       " 'Tom Crean': 382,\n",
       " 'Tom Cruise': 383,\n",
       " 'Tom Daschle': 384,\n",
       " 'Tom Hanks': 385,\n",
       " 'Tom Harkin': 386,\n",
       " 'Tom Ridge': 387,\n",
       " 'Tommy Franks': 388,\n",
       " 'Tommy Haas': 389,\n",
       " 'Tommy Thompson': 390,\n",
       " 'Tony Blair': 391,\n",
       " 'Tony Stewart': 392,\n",
       " 'Trent Lott': 393,\n",
       " 'Tung Chee-hwa': 394,\n",
       " 'Vaclav Havel': 395,\n",
       " 'Valentino Rossi': 396,\n",
       " 'Valery Giscard dEstaing': 397,\n",
       " 'Vanessa Redgrave': 398,\n",
       " 'Venus Williams': 399,\n",
       " 'Vicente Fernandez': 400,\n",
       " 'Vicente Fox': 401,\n",
       " 'Victoria Clarke': 402,\n",
       " 'Vincent Brooks': 403,\n",
       " 'Vladimir Putin': 404,\n",
       " 'Vojislav Kostunica': 405,\n",
       " 'Walter Mondale': 406,\n",
       " 'Wayne Ferreira': 407,\n",
       " 'Wen Jiabao': 408,\n",
       " 'William Donaldson': 409,\n",
       " 'William Ford Jr': 410,\n",
       " 'William Macy': 411,\n",
       " 'Winona Ryder': 412,\n",
       " 'Woody Allen': 413,\n",
       " 'Xanana Gusmao': 414,\n",
       " 'Xavier Malisse': 415,\n",
       " 'Yao Ming': 416,\n",
       " 'Yashwant Sinha': 417,\n",
       " 'Yasser Arafat': 418,\n",
       " 'Yoko Ono': 419,\n",
       " 'Yoriko Kawaguchi': 420,\n",
       " 'Zhu Rongji': 421,\n",
       " 'Zinedine Zidane': 422}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_naked, train_masked, test_naked, test_masked\n",
    "# trainA, trainB, testA, testB respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ZIkvewRiAe5s"
   },
   "outputs": [],
   "source": [
    "dset_trainA = PandasDataset(trainA, transform=data_transforms[\"train\"], target_transform=tokenize_names)\n",
    "dset_testA = PandasDataset(testA, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "\n",
    "dl_trainA = torch.utils.data.DataLoader(dset_trainA, batch_size=32, shuffle=True, num_workers=4)\n",
    "dl_testA = torch.utils.data.DataLoader(dset_testA, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": dl_trainA, \"val\": dl_testA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/imad/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de5eda7b9f7474eaf13082e47dc6801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "if type(model) == torchvision.models.vgg.VGG:\n",
    "  num_ftrs = model.classifier[6].in_features\n",
    "  model.classifier[6] = nn.Linear(num_ftrs, len(name_to_idx))\n",
    "else:\n",
    "  num_ftrs = model.fc.in_features\n",
    "  model.fc = nn.Linear(num_ftrs, len(name_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBWCVREmLs_F",
    "outputId": "e687d205-9352-406a-9560-51492563e5de"
   },
   "outputs": [],
   "source": [
    "import time, copy, os\n",
    "\n",
    "def train_model(model, criterion, dataloaders, optimizer, scheduler, num_epochs=25, cuda_id = 0):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    logs = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.cuda(cuda_id)\n",
    "                labels = labels.cuda(cuda_id)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            logs.append('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_acc_history, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "8mt2VTIsjUI8",
    "outputId": "2d91268a-3819-42a6-a570-583630b22c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.2482 Acc: 0.0016\n",
      "val Loss: 6.1762 Acc: 0.0000\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 6.1465 Acc: 0.0016\n",
      "val Loss: 6.1002 Acc: 0.0059\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 6.0437 Acc: 0.0024\n",
      "val Loss: 6.0365 Acc: 0.0059\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 5.9599 Acc: 0.0118\n",
      "val Loss: 5.9730 Acc: 0.0106\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 5.8804 Acc: 0.0189\n",
      "val Loss: 5.9236 Acc: 0.0142\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 5.8050 Acc: 0.0205\n",
      "val Loss: 5.8576 Acc: 0.0189\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 5.7091 Acc: 0.0433\n",
      "val Loss: 5.7669 Acc: 0.0331\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 5.6048 Acc: 0.0709\n",
      "val Loss: 5.6938 Acc: 0.0402\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 5.5098 Acc: 0.0867\n",
      "val Loss: 5.6174 Acc: 0.0473\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 5.4074 Acc: 0.1017\n",
      "val Loss: 5.5182 Acc: 0.0496\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 5.3013 Acc: 0.1560\n",
      "val Loss: 5.4293 Acc: 0.0709\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 5.2001 Acc: 0.1600\n",
      "val Loss: 5.3340 Acc: 0.0863\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 5.1047 Acc: 0.1899\n",
      "val Loss: 5.2631 Acc: 0.0969\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.9888 Acc: 0.2183\n",
      "val Loss: 5.1548 Acc: 0.1229\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 4.8776 Acc: 0.2703\n",
      "val Loss: 5.0610 Acc: 0.1241\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 4.7633 Acc: 0.3002\n",
      "val Loss: 4.9783 Acc: 0.1371\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 4.6322 Acc: 0.3444\n",
      "val Loss: 4.8574 Acc: 0.1631\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 4.5143 Acc: 0.3507\n",
      "val Loss: 4.7871 Acc: 0.1690\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 4.4153 Acc: 0.4058\n",
      "val Loss: 4.6966 Acc: 0.1797\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 4.3055 Acc: 0.4232\n",
      "val Loss: 4.6113 Acc: 0.2021\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 4.1976 Acc: 0.4374\n",
      "val Loss: 4.5363 Acc: 0.2033\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 4.0610 Acc: 0.4854\n",
      "val Loss: 4.4520 Acc: 0.2305\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 3.9609 Acc: 0.5303\n",
      "val Loss: 4.3874 Acc: 0.2435\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 3.8518 Acc: 0.5422\n",
      "val Loss: 4.2806 Acc: 0.2518\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 3.7181 Acc: 0.5887\n",
      "val Loss: 4.2313 Acc: 0.2683\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 3.6445 Acc: 0.5950\n",
      "val Loss: 4.1384 Acc: 0.2896\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 3.5207 Acc: 0.6344\n",
      "val Loss: 4.0658 Acc: 0.2943\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 3.4144 Acc: 0.6517\n",
      "val Loss: 3.9737 Acc: 0.2955\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 3.3212 Acc: 0.6619\n",
      "val Loss: 3.9339 Acc: 0.3156\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 3.2311 Acc: 0.6777\n",
      "val Loss: 3.8567 Acc: 0.3310\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 3.1216 Acc: 0.7013\n",
      "val Loss: 3.8043 Acc: 0.3440\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 3.0328 Acc: 0.7234\n",
      "val Loss: 3.7518 Acc: 0.3440\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 2.9163 Acc: 0.7549\n",
      "val Loss: 3.6958 Acc: 0.3582\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 2.8039 Acc: 0.7683\n",
      "val Loss: 3.6509 Acc: 0.3676\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 2.7043 Acc: 0.7943\n",
      "val Loss: 3.5956 Acc: 0.3712\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 2.6676 Acc: 0.7715\n",
      "val Loss: 3.5350 Acc: 0.3865\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 2.5403 Acc: 0.8046\n",
      "val Loss: 3.4740 Acc: 0.3948\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 2.4409 Acc: 0.8061\n",
      "val Loss: 3.4185 Acc: 0.4078\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 2.3463 Acc: 0.8337\n",
      "val Loss: 3.3880 Acc: 0.4019\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 2.2687 Acc: 0.8353\n",
      "val Loss: 3.2992 Acc: 0.4090\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 2.2079 Acc: 0.8597\n",
      "val Loss: 3.2664 Acc: 0.4326\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 2.1120 Acc: 0.8582\n",
      "val Loss: 3.2436 Acc: 0.4385\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 2.0095 Acc: 0.8857\n",
      "val Loss: 3.1686 Acc: 0.4444\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.9435 Acc: 0.8842\n",
      "val Loss: 3.1627 Acc: 0.4350\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.8851 Acc: 0.8897\n",
      "val Loss: 3.1304 Acc: 0.4385\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.7862 Acc: 0.8999\n",
      "val Loss: 3.0844 Acc: 0.4468\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.7142 Acc: 0.9117\n",
      "val Loss: 3.0454 Acc: 0.4515\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.6587 Acc: 0.9180\n",
      "val Loss: 3.0145 Acc: 0.4563\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.6071 Acc: 0.9039\n",
      "val Loss: 2.9710 Acc: 0.4574\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.5308 Acc: 0.9314\n",
      "val Loss: 2.9425 Acc: 0.4622\n",
      "\n",
      "Training complete in 4m 23s\n",
      "Best val Acc: 0.462175\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "\n",
    "model = train_model(model, criterion, dataloaders, optimizer, exp_lr_scheduler, num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "HnWrV15sla4Z",
    "outputId": "42bcc43a-5d26-4ebb-fda2-5d6ffd216c4a"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'pretrained/imad_resnet_naked.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naked\n",
    "\n",
    "type_model = \"imad_resnet_\"\n",
    "### Load Model\n",
    "dir_pretrained = os.path.join(root_path, \"pretrained\")\n",
    "model_name = type_model + \"naked.pt\"\n",
    "model_path = os.path.join(dir_pretrained, model_name)\n",
    "\n",
    "model, scores, history = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results from CycleGAN\n",
    "cycle_gan_folder = \"pytorch-CycleGAN-and-pix2pix\"\n",
    "dataset = \"LFW Dataset\"\n",
    "model_trained = \"mask2unmask_final2\"\n",
    "\n",
    "fake_naked_path = os.path.join(root_path, cycle_gan_folder, \"results\", model_trained, \"fake_naked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(folder_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "        for name in files:\n",
    "            paths.append((root, name))\n",
    "    return paths\n",
    "    \n",
    "def create_df(paths):\n",
    "    df = pd.DataFrame(paths, columns=['root', 'filename'])\n",
    "    \n",
    "    df['fullpath'] = df['root'] + '/' + df['filename']\n",
    "    df = df.drop(columns=\"root\")\n",
    "    df[\"fake\"]   = False\n",
    "    df['person'] = df['filename'].apply(lambda x: \" \".join(x.split(\"_\")[:-2]))\n",
    "    \n",
    "    df['status'] = df['filename'].apply(lambda x: x.split(\"_\")[-1][:-4])\n",
    "    df[\"name\"] = df[\"filename\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "    df['fake'].loc[df['status'] == 'fake'] = True\n",
    "    df = df.drop(columns=\"status\")\n",
    "    \n",
    "    fake_df = df[df['fake']]\n",
    "    fake_df = fake_df.drop(columns=\"fake\")\n",
    "    \n",
    "    real_df = df[~df['fake']]\n",
    "    real_df = real_df.drop(columns=\"fake\")\n",
    "    \n",
    "    return real_df, fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting results from CycleGAN\n",
    "\n",
    "fake_naked = extract_files(fake_naked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imad/anaconda3/envs/inf664/lib/python3.8/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "fake_naked_real, fake_naked_fake = create_df(fake_naked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naked Fake dataloader\n",
    "ds_fake_naked_fake = PandasDataset(fake_naked_fake, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "dl_fake_naked_fake = torch.utils.data.DataLoader(ds_fake_naked_fake, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fa16af0c940>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_fake_naked_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_accuracy(dl_, model):\n",
    "    match = 0\n",
    "    with torch.no_grad():\n",
    "        for input, label in tqdm(dl_):\n",
    "            input = input.cuda()\n",
    "            out = model(input)\n",
    "            pred_y = torch.argmax(out, dim=-1).cpu()\n",
    "            match += torch.sum(pred_y == label)\n",
    "\n",
    "    n_ = len(dl_.dataset)\n",
    "    print('\\n\\tNumber of matches: {}, Number of samples: {}, Accuracy: {:.4f}'.format(match, n_, match/n_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tNumber of matches: 161, Number of samples: 500, Accuracy: 0.3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Naked Fake\n",
    "test_accuracy(dl_fake_naked_fake, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "if type(model) == torchvision.models.vgg.VGG:\n",
    "  num_ftrs = model.classifier[6].in_features\n",
    "  model.classifier[6] = nn.Linear(num_ftrs, len(name_to_idx))\n",
    "else:\n",
    "  num_ftrs = model.fc.in_features\n",
    "  model.fc = nn.Linear(num_ftrs, len(name_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainB[\"person\"].unique()) == len(testB[\"person\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_trainB = PandasDataset(trainB, transform=data_transforms[\"train\"], target_transform=tokenize_names)\n",
    "dset_testB = PandasDataset(testB, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "\n",
    "dl_trainB = torch.utils.data.DataLoader(dset_trainB, batch_size=32, shuffle=True, num_workers=4)\n",
    "dl_testB = torch.utils.data.DataLoader(dset_testB, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": dl_trainB, \"val\": dl_testB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.2623 Acc: 0.0008\n",
      "val Loss: 6.2234 Acc: 0.0013\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 6.1582 Acc: 0.0024\n",
      "val Loss: 6.1329 Acc: 0.0037\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 6.0415 Acc: 0.0032\n",
      "val Loss: 6.0925 Acc: 0.0037\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 5.9560 Acc: 0.0071\n",
      "val Loss: 6.0409 Acc: 0.0050\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 5.8963 Acc: 0.0102\n",
      "val Loss: 5.9984 Acc: 0.0075\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 5.8163 Acc: 0.0252\n",
      "val Loss: 5.9448 Acc: 0.0100\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 5.7308 Acc: 0.0370\n",
      "val Loss: 5.8882 Acc: 0.0112\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 5.6294 Acc: 0.0575\n",
      "val Loss: 5.8395 Acc: 0.0163\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 5.5603 Acc: 0.0709\n",
      "val Loss: 5.7775 Acc: 0.0213\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 5.4290 Acc: 0.0930\n",
      "val Loss: 5.7111 Acc: 0.0200\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 5.3304 Acc: 0.1221\n",
      "val Loss: 5.6550 Acc: 0.0300\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 5.2423 Acc: 0.1647\n",
      "val Loss: 5.5774 Acc: 0.0312\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 5.1397 Acc: 0.2033\n",
      "val Loss: 5.5264 Acc: 0.0375\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 5.0394 Acc: 0.2254\n",
      "val Loss: 5.4682 Acc: 0.0500\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 4.9370 Acc: 0.2514\n",
      "val Loss: 5.3973 Acc: 0.0550\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 4.8107 Acc: 0.2971\n",
      "val Loss: 5.3352 Acc: 0.0625\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 4.7180 Acc: 0.3105\n",
      "val Loss: 5.2806 Acc: 0.0638\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 4.6062 Acc: 0.3452\n",
      "val Loss: 5.2242 Acc: 0.0638\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 4.5056 Acc: 0.3696\n",
      "val Loss: 5.1548 Acc: 0.0912\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 4.3662 Acc: 0.4192\n",
      "val Loss: 5.0931 Acc: 0.0912\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 4.2796 Acc: 0.4531\n",
      "val Loss: 5.0302 Acc: 0.1013\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 4.1706 Acc: 0.4704\n",
      "val Loss: 4.9694 Acc: 0.1113\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 4.0722 Acc: 0.5067\n",
      "val Loss: 4.9355 Acc: 0.1250\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 3.9524 Acc: 0.5296\n",
      "val Loss: 4.8760 Acc: 0.1188\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 3.8323 Acc: 0.5682\n",
      "val Loss: 4.8274 Acc: 0.1375\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 3.7279 Acc: 0.5729\n",
      "val Loss: 4.7859 Acc: 0.1425\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 3.6611 Acc: 0.6028\n",
      "val Loss: 4.7357 Acc: 0.1562\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 3.5395 Acc: 0.6422\n",
      "val Loss: 4.6752 Acc: 0.1725\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 3.4286 Acc: 0.6541\n",
      "val Loss: 4.6127 Acc: 0.1750\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 3.3349 Acc: 0.6572\n",
      "val Loss: 4.5689 Acc: 0.1663\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 3.2716 Acc: 0.6722\n",
      "val Loss: 4.5334 Acc: 0.1862\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 3.1049 Acc: 0.7092\n",
      "val Loss: 4.4906 Acc: 0.1800\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 3.0292 Acc: 0.7321\n",
      "val Loss: 4.4577 Acc: 0.1938\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 2.9614 Acc: 0.7187\n",
      "val Loss: 4.4267 Acc: 0.2025\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 2.8375 Acc: 0.7699\n",
      "val Loss: 4.3731 Acc: 0.1950\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 2.7362 Acc: 0.7801\n",
      "val Loss: 4.3406 Acc: 0.2125\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 2.6898 Acc: 0.7746\n",
      "val Loss: 4.2740 Acc: 0.2263\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 2.5556 Acc: 0.8109\n",
      "val Loss: 4.2460 Acc: 0.2238\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 2.4845 Acc: 0.8235\n",
      "val Loss: 4.2365 Acc: 0.2225\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 2.4337 Acc: 0.8101\n",
      "val Loss: 4.1876 Acc: 0.2338\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 2.3149 Acc: 0.8400\n",
      "val Loss: 4.1451 Acc: 0.2462\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 2.2381 Acc: 0.8432\n",
      "val Loss: 4.1203 Acc: 0.2538\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 2.1700 Acc: 0.8479\n",
      "val Loss: 4.0570 Acc: 0.2538\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 2.1012 Acc: 0.8700\n",
      "val Loss: 4.0725 Acc: 0.2350\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.9797 Acc: 0.8881\n",
      "val Loss: 4.0296 Acc: 0.2662\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.8896 Acc: 0.8826\n",
      "val Loss: 4.0159 Acc: 0.2538\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.8576 Acc: 0.8983\n",
      "val Loss: 3.9714 Acc: 0.2637\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.7651 Acc: 0.9070\n",
      "val Loss: 3.9473 Acc: 0.2725\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.7059 Acc: 0.9046\n",
      "val Loss: 3.9269 Acc: 0.2712\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.6275 Acc: 0.9236\n",
      "val Loss: 3.9131 Acc: 0.2812\n",
      "\n",
      "Training complete in 4m 15s\n",
      "Best val Acc: 0.281250\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "model = train_model(model, criterion, dataloaders, optimizer, exp_lr_scheduler, num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'pretrained/imad_resnet_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Masked\n",
    "\n",
    "type_model = \"imad_resnet_\"\n",
    "### Load Model\n",
    "dir_pretrained = os.path.join(root_path, \"pretrained\")\n",
    "model_name = type_model + \"masked.pt\"\n",
    "model_path = os.path.join(dir_pretrained, model_name)\n",
    "\n",
    "model, scores, history = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results from CycleGAN\n",
    "cycle_gan_folder = \"pytorch-CycleGAN-and-pix2pix\"\n",
    "dataset = \"LFW Dataset\"\n",
    "model_trained = \"mask2unmask_final2\"\n",
    "\n",
    "fake_masked_path = os.path.join(root_path, cycle_gan_folder, \"results\", model_trained, \"fake_masked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting results from CycleGAN\n",
    "\n",
    "fake_masked = extract_files(fake_masked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fake_masked_fake = create_df(fake_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_fake_masked = PandasDataset(fake_masked_fake, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "dl_fake_masked = torch.utils.data.DataLoader(dset_fake_masked, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tNumber of matches: 103, Number of samples: 500, Accuracy: 0.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Naked Masked\n",
    "test_accuracy(dl_fake_masked, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naked\n",
    "\n",
    "type_model = \"imad_resnet_\"\n",
    "### Load Model\n",
    "dir_pretrained = os.path.join(root_path, \"pretrained\")\n",
    "model_name = type_model + \"general.pt\"\n",
    "model_path = os.path.join(dir_pretrained, model_name)\n",
    "\n",
    "model, scores, history = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results from CycleGAN\n",
    "cycle_gan_folder = \"pytorch-CycleGAN-and-pix2pix\"\n",
    "dataset = \"LFW Dataset\"\n",
    "model_trained = \"mask2unmask_final2\"\n",
    "\n",
    "fake_masked_path = os.path.join(root_path, cycle_gan_folder, \"results\", model_trained, \"fake_masked\")\n",
    "fake_naked_path = os.path.join(root_path, cycle_gan_folder, \"results\", model_trained, \"fake_naked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting results from CycleGAN\n",
    "\n",
    "fake_naked = extract_files(fake_naked_path)\n",
    "fake_masked = extract_files(fake_masked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fake_naked_fake = create_df(fake_naked)\n",
    "_, fake_masked_fake = create_df(fake_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_all = pd.concat([fake_naked_fake, fake_masked_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_fake_all = PandasDataset(df_fake_all, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "dl_fake_all = torch.utils.data.DataLoader(dset_fake_all, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tNumber of matches: 457, Number of samples: 1000, Accuracy: 0.4570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#General model\n",
    "test_accuracy(dl_fake_all, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data on each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testA: Naked\n",
    "dset_fake_naked = PandasDataset(testA, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "dl_fake_naked = torch.utils.data.DataLoader(dset_fake_naked, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:02<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tNumber of matches: 423, Number of samples: 846, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#General model on Faked Nake\n",
    "test_accuracy(dl_fake_naked, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testB: Masked\n",
    "dset_fake_masked = PandasDataset(testB, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "dl_fake_masked = torch.utils.data.DataLoader(dset_fake_masked, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tNumber of matches: 395, Number of samples: 800, Accuracy: 0.4938\n"
     ]
    }
   ],
   "source": [
    "#General model on Faked Nake\n",
    "test_accuracy(dl_fake_masked, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/imad/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24547432f0a243b8a30df4e469ff6cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "if type(model) == torchvision.models.vgg.VGG:\n",
    "  num_ftrs = model.classifier[6].in_features\n",
    "  model.classifier[6] = nn.Linear(num_ftrs, len(name_to_idx))\n",
    "else:\n",
    "  num_ftrs = model.fc.in_features\n",
    "  model.fc = nn.Linear(num_ftrs, len(name_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_trainB = PandasDataset(trainB, transform=data_transforms[\"train\"], target_transform=tokenize_names)\n",
    "dset_testB = PandasDataset(testB, transform=data_transforms[\"val\"], target_transform=tokenize_names)\n",
    "\n",
    "dl_trainB = torch.utils.data.DataLoader(dset_trainB, batch_size=32, shuffle=True, num_workers=4)\n",
    "dl_testB = torch.utils.data.DataLoader(dset_testB, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": dl_trainB, \"val\": dl_testB}\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "model = model.cuda(1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.1172 Acc: 0.0016\n",
      "val Loss: 6.0530 Acc: 0.0037\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 6.0480 Acc: 0.0032\n",
      "val Loss: 6.0458 Acc: 0.0050\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 6.0382 Acc: 0.0071\n",
      "val Loss: 6.0377 Acc: 0.0025\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 6.0153 Acc: 0.0079\n",
      "val Loss: 6.0257 Acc: 0.0037\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 5.9900 Acc: 0.0102\n",
      "val Loss: 5.9995 Acc: 0.0063\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 5.9486 Acc: 0.0055\n",
      "val Loss: 5.9411 Acc: 0.0063\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 5.8513 Acc: 0.0055\n",
      "val Loss: 5.7823 Acc: 0.0138\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 5.7073 Acc: 0.0102\n",
      "val Loss: 5.6488 Acc: 0.0150\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 5.5627 Acc: 0.0142\n",
      "val Loss: 5.4450 Acc: 0.0375\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 5.3678 Acc: 0.0418\n",
      "val Loss: 5.3812 Acc: 0.0475\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 5.0593 Acc: 0.0662\n",
      "val Loss: 5.1457 Acc: 0.0600\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 4.8430 Acc: 0.0835\n",
      "val Loss: 5.0587 Acc: 0.0663\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 4.5148 Acc: 0.1119\n",
      "val Loss: 4.9816 Acc: 0.0762\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.1645 Acc: 0.1521\n",
      "val Loss: 4.7364 Acc: 0.1050\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 3.8470 Acc: 0.2017\n",
      "val Loss: 4.7251 Acc: 0.1013\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 3.6184 Acc: 0.2372\n",
      "val Loss: 4.7061 Acc: 0.1163\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 3.2404 Acc: 0.2955\n",
      "val Loss: 4.5329 Acc: 0.1300\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 3.0727 Acc: 0.3113\n",
      "val Loss: 4.6192 Acc: 0.1325\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 2.6426 Acc: 0.4027\n",
      "val Loss: 4.4761 Acc: 0.1575\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 2.7557 Acc: 0.3672\n",
      "val Loss: 4.5693 Acc: 0.1487\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 2.2626 Acc: 0.4697\n",
      "val Loss: 4.7603 Acc: 0.1487\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 2.0431 Acc: 0.5059\n",
      "val Loss: 5.0126 Acc: 0.1500\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 2.0171 Acc: 0.5067\n",
      "val Loss: 5.1223 Acc: 0.1562\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.8100 Acc: 0.5634\n",
      "val Loss: 4.8334 Acc: 0.1450\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.8336 Acc: 0.5658\n",
      "val Loss: 4.9510 Acc: 0.1825\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.6722 Acc: 0.5650\n",
      "val Loss: 4.9484 Acc: 0.1812\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.4730 Acc: 0.6186\n",
      "val Loss: 5.3235 Acc: 0.1688\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.5046 Acc: 0.6273\n",
      "val Loss: 4.7321 Acc: 0.1837\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.2182 Acc: 0.6801\n",
      "val Loss: 5.1218 Acc: 0.1812\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.2160 Acc: 0.6832\n",
      "val Loss: 5.0551 Acc: 0.1837\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.1090 Acc: 0.7147\n",
      "val Loss: 5.1452 Acc: 0.1975\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.0408 Acc: 0.7266\n",
      "val Loss: 5.1746 Acc: 0.1875\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.9669 Acc: 0.7439\n",
      "val Loss: 5.2871 Acc: 0.1825\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.8994 Acc: 0.7526\n",
      "val Loss: 5.7258 Acc: 0.1950\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.9802 Acc: 0.7486\n",
      "val Loss: 5.4073 Acc: 0.1787\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.8106 Acc: 0.7825\n",
      "val Loss: 5.5754 Acc: 0.1975\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.8478 Acc: 0.7723\n",
      "val Loss: 5.4696 Acc: 0.2087\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.7761 Acc: 0.7967\n",
      "val Loss: 5.4630 Acc: 0.1825\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.7954 Acc: 0.7849\n",
      "val Loss: 5.5457 Acc: 0.2000\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.6718 Acc: 0.8243\n",
      "val Loss: 5.3906 Acc: 0.2087\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.8385\n",
      "val Loss: 5.5476 Acc: 0.2175\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.8392\n",
      "val Loss: 5.2371 Acc: 0.2050\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.6398 Acc: 0.8258\n",
      "val Loss: 5.4088 Acc: 0.1950\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.5772 Acc: 0.8337\n",
      "val Loss: 5.9166 Acc: 0.1875\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.6361 Acc: 0.8329\n",
      "val Loss: 5.3142 Acc: 0.2075\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 0.8786\n",
      "val Loss: 5.7419 Acc: 0.1950\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.5022 Acc: 0.8629\n",
      "val Loss: 5.7125 Acc: 0.1988\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.4884 Acc: 0.8660\n",
      "val Loss: 5.5385 Acc: 0.2263\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.4661 Acc: 0.8763\n",
      "val Loss: 6.0109 Acc: 0.2013\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.5236 Acc: 0.8511\n",
      "val Loss: 5.5071 Acc: 0.2200\n",
      "\n",
      "Training complete in 9m 10s\n",
      "Best val Acc: 0.226250\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "model = train_model(model, criterion, dataloaders, optimizer, exp_lr_scheduler, num_epochs = 50, cuda_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'pretrained/imad_vgg_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Old version of transfer_pytorch_good.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "inf664",
   "language": "python",
   "name": "inf664"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
